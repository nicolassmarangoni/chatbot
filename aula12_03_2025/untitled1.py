# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lCcJW74fH0U3w6RkwaCS8iGF33tHdi0R
"""

#Desenvolvimento de um chatbot especialista para venda

!pip install -q llama-index

#bilbioteca para permitir a leitura de arquvios de pdf
from llama_index.core import SimpleDirectoryReader
documentos = SimpleDirectoryReader(input_dir='documentos')

documentos.input_files

docs = documentos.load_data() # armazena o documento

len(docs)

print(docs[0].get_metadata_str())

docs[0].__dict__

#importando a biblioteca
from llama_index.core.node_parser import SentenceSplitter
node_parser= SentenceSplitter(chunk_size=1200)

nodes = node_parser.get_nodes_from_documents(docs,show_progress=True)

len(nodes)

nodes[0]

nodes[9]

#instalação do hugging face
!pip install -q llama-index-embeddings-huggingface

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

#Classe personalizada para integrar ao Chroma DB

class ChromaEmbeddingWrapper:
  def __init__(self,model_name:str):
    self.model = HuggingFaceEmbedding(model_name= model_name)

  def __call__(self,input):
    return self.model.embed(input)

embed_model_chroma= ChromaEmbeddingWrapper(model_name='intfloat/multilingual-e5-large')

#instalação do chroma db
!pip install -q llama-index-vector-stores-chroma

#importando chroma db

import chromadb
db = chromadb.PersistentClient(path='./chroma_db')
chroma_client = db
collection_name = 'documentos_serenatto'

try:
  chroma_collection = chroma_client.get_or_create_collection(
      name=collection_name,
      embedding_function=embed_model_chroma)

except Exception as e :
  print(f'Erro ao criar coleção {e}')

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

#criando a coleção de banco de dados
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

embed_model = HuggingFaceEmbedding(model_name ='intfloat/multilingual-e5-large')

from llama_index.core import VectorStoreIndex

index = VectorStoreIndex(nodes, storage_context=storage_context,embed_model = embed_model)

from llama_index.core import load_index_from_storage
index = load_index_from_storage(storage_context, embed_model=embed_model)

from google.colab import userdata
Groq_api = userdata.get('api_groq_chat')

!pip install -q llama-index-llms-groq

from llama_index.llms.groq import Groq

llms = Groq(model = 'llama3-70b-8192',api_key=Groq_api )

query_engine = index.as_query_engine(llm=llms,similarity_top_k=2)

query_engine.query('Quais graos estao disponiveis').response

query_embedding = embed_model.get_text_embedding('Quais graos estao disponiveis ?')

chroma_collection.query(query_embedding,n_results=2,include=['distances','embeddings'])

chat_engine = index.as_chat_engine(mode='context',llm = llms)

pergunta = chat_engine.chat('Quais graos estao disponiveis ?').response
print(pergunta)

pergunta = chat_engine.chat('Quais sao os preços dos graos ?').response
print(pergunta)

pergunta = chat_engine.chat('Voce pode me dar mais detalhes sobre o Catuai amarelo ?').response
print(pergunta)

pergunta = chat_engine.chat('Qual o preço dele ?').response
print(pergunta)

chat_engine.chat_history

from llama_index.core.memory import ChatSummaryMemoryBuffer

memory = ChatSummaryMemoryBuffer(llm=llms,token_limit=256)

chat_engine= index.as_chat_engine(
    chat_mode = 'context',
    llm=llms,
    memory=memory,
    system_prompt=('''Voce é especialista em cafes da loja Serenato, uma loja online que vende graos de cafe
   torrados, sua funçao é tirar duvidas de forma simpatica e natural sobre os graos disponiveis''')
)

response = chat_engine.chat('Ola')
print(response)

response = chat_engine.chat('Voce pode me dar mais detalhes sobre o catui amarelo')
print(response)

response = chat_engine.chat('qual o preço dele')
print(response)

memory.get()

#resetando o chat
chat_engine.reset()

response = chat_engine.chat('qual o preço dele')
print(response)

# implementando a interface com o Gradio
!pip install gradio

import gradio as gr

# Criando a função para conversar com o chatbot
def converse_com_bot(message, chat_history):
    response = chat_engine.chat(message)

    if chat_history is None:
        chat_history = []

    chat_history.append({"role": "user", "content": message})
    chat_history.append({"role": "assistant", "content": response.response})

    return "", chat_history

# Criando a função para resetar o chatbot
def resetar_chat():
    chat_engine.reset()
    return []

with gr.Blocks() as app:

    gr.Markdown('# Chatbot da Serenatto')
    chatbot = gr.Chatbot(type='messages')
    msg = gr.Textbox(label='Digite a sua mensagem')
    limpar = gr.Button('Limpar')

    msg.submit(converse_com_bot,[msg,chatbot],[msg,chatbot])
    limpar.click(resetar_chat,None,chatbot,queue=False)

    app.launch(debug=True)